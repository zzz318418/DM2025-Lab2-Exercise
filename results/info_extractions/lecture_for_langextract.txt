# LECTURE ANALYSIS: Data Sets
## Video Segment: 110:58 - 114:58

[SLIDE DESCRIPTION]
The slide defines Data Sets as a collection of data organized in certain formats. It further categorizes them into Structured Data Sets, which are often in rows and columns and have a clear, predictable structure, and Unstructured Data Sets, which are more complex and varied, usually derived from human-generated content like text, images, and videos. The speaker clarifies that structured data means you know the exact meaning of rows and columns, while unstructured data can be difficult to interpret. Examples of unstructured data include text, images, and videos, which are often generated by humans. The speaker notes that while AI is improving, converting unstructured data to structured format is still a challenge.

[TRANSCRIPT - 110:58]
So the next one we want to talking about the data set. We have different type of data set. You might heard about the term called structured data or unstructured data. Structured data is you know exactly the meaning about the row and column. Unstructured data is they will change. Sometimes you don't really know how to interpret them. Currently that the human generate content such as image, text, video, we consider them as unstructured data. Luckily with this AI thing and right now text, image, video already can be converting into structured format, but you still need to be able to know.

[SLIDE DESCRIPTION]
The slide lists characteristics of Data Sets: Dimensionality (Curse of Dimensionality), Sparsity (Only presence counts), and Resolution (Patterns depend on the scale). The speaker elaborates on Sparsity, explaining it comes from the word 'sparse' meaning 'very few'. An example is given of an e-commerce site like Amazon, where a user might have millions of records of viewed items but only purchase a few, making their data 'sparse'. The speaker mentions that they will discuss resolution and sparsity in more detail later, possibly in classification or text mining sections.

[TRANSCRIPT - 111:58]
So we have different kind of characteristic about our data dimension we mentioned before. Sparsity comes from a key word called sparse. Sparse means very few. Like what we say that if you have the record in e-commerce store, like in Amazon. Your data is sparse in that data record. Because they have millions of the record in the Amazon. But then you probably only buy 10 books before. So make your data in the millions then become very sparse. So data are sparse. And sometimes in certain place, we will also talking about the resolution. And that part about sparsity, how to solving them, how to do the resolution, we will talking about later in some of the courses, some of them will be in classification, some of them will be in the text mining.

[SLIDE DESCRIPTION]
The slide introduces Types of Data Sets: Data Matrix, Document Data, and Transaction Data. The speaker explains Data Matrix: if each object has the same fixed set of numeric attributes, they can become data metrics and allow for ratio operations. The example shows books with attributes like Price, Sells, Reviews, and Readers. The speaker emphasizes that for data to be a data matrix, all values must be numeric and allow for ratio operations. If not, the matrix is considered 'wrong'.

[TRANSCRIPT - 112:58]
We have different type of the data set. We have the data matrix, document data, transaction data. So data matrix look like this way. If your data can be convert into meaningful number, they can become data metric. And then we allow them to do the ratio operation. They are called data matrix. To make sure all the value inside need to become the ratio data. So then right here they need to be ratio data.

[SLIDE DESCRIPTION]
The slide discusses Document Data, describing it as usually unstructured and textual. Each term is a component (attribute) of the vector, and the value is the count of the term's occurrences. An example table shows 'Document 1', 'Document 2', and 'Document 3' with counts for terms like 'teach', 'coach', 'learn', 'score', 'game', 'score', 'teach', 'coach', 'learn'. The speaker explains that before 2013, document data was often utilized in a vector form by selecting important keywords and generating a 'term vector' based on the frequency of those keywords. This is because in computer science, vectors are a convenient way to operate on data.

[TRANSCRIPT - 113:48]
And the document data is a part that we were talking about later because before 2013, this is before 2013. Okay. And the people actually utilize document data in a vector form. Right here. So they select important keyword in the document. And then after that for every document, you utilize those important keyword, we got some number, occurrence of the number for that keyword right there. We generate a something called term vector. So each one of them are vector. Why vector? Because in computer science, vector is the easy way for us to operate. We convert them to vector. So in other word, every number right here are become ratio data. Okay, but that is way we try to work in that before 2013. After 2013, how we try to do that, we will talking about that that in the text mining parts. So maybe three weeks later.
